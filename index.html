<!DOCTYPE html>
<html>

<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>bigFileUploadDemo</title>
</head>

<body>

  <input id="uploadFile" type="file" />
  <button id="submit" onclick="upload()">上传文件</button>

</body>

<!-- <script src="/learnBlob.js"></script> -->

<script src="/spark-md5.min.js"></script>

<script>
  const uploadFileEle = document.querySelector("#uploadFile");
  const MB = 1048576
  const chunkSize = 5 * MB// 单位MB


  async function upload() {
    const [file] = uploadFileEle.files
    const fileInfo = await processFile(file)
    console.log(fileInfo)
    fetchUpload(fileInfo.chunkList)
  }

  async function fetchUpload(chunkList) {
    const formdata = new FormData()
    chunkList.forEach(item => {
      formdata.append('chunk', item.chunk, item.hash)
    })
    const res = await fetch(`http://127.0.0.1:3000/upload`, {
      method: 'POST',
      body: formdata
    }).then(res => res.json())
    console.log(res)
  }

  async function fetchMerge(file) {
    const res = await fetch('http://127.0.0.1:3000/merge')
      .then(res => res.json())
    console.log(res)
  }

  //文件处理函数
  async function processFile(file) {
    chunkCount = Math.ceil(file.size / (chunkSize))
    const chunkList = sliceFile({ file, chunkCount, chunkSize })

    //通过promise来处理回调保证结果顺序
    const finalFile = (await Promise.all([
      //完整文件的hash
      { hash: await computeHash(file), file, type: 'complete' },
      //分块文件的hash
      ...chunkList.map(async (chunk, chunkIndex) =>
        ({ hash: await computeHash(chunk), chunk, chunkIndex })
      )
    ]))//换一种数据结构来存储
      .reduce((acc, fileItem) => {
        if (fileItem.type === 'complete') {
          acc = fileItem
          acc.chunkList = []
        } else {
          acc.chunkList.push(fileItem)
        }
        return acc
      }, {})
    return finalFile
  }

  //切片函数
  function sliceFile({ file, chunkCount, chunkSize }) {
    return new Array(chunkCount).fill().map((_, index) => file.slice(index * chunkSize, (index + 1) * chunkSize))
  }

  //计算单个文件哈希
  function computeHash(file) {
    //使用SparkMD5暴露的ArrayBuffer接口来获取哈希
    const spark = new SparkMD5.ArrayBuffer()
    return new Promise((resolve, reject) => {
      const reader = new FileReader()
      //读取blob格式文件
      reader.readAsArrayBuffer(file)
      //读取blob成ArrayBuffer时的回调
      reader.onload = (e) => {
        //将ArrayBuffer添加给spark
        spark.append(e.target.result)
        //结束添加，并计算得到hash值
        resolve(spark.end())
      }
      reader.onerror = (e) => {
        reject(e)
        reader.abort();
      }
    })
  }

</script>

</html>